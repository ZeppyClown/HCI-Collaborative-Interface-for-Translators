### ğŸš¦Quick alignment before we dive into code

Below is a **one-screen recap** of the agreed pipeline. Tell me which block you want to implement first and Iâ€™ll jump straight into concrete code with you.

---

#### 1ï¸âƒ£ Translate + Log-probs

- **OpenAI ChatCompletion**

  - `logprobs=True`, `top_logprobs=K` (e.g. 5)
  - Output: English draft **+** per-token log-prob table

#### 2ï¸âƒ£ Chunk the draft

- **spaCy (`en_core_web_trf`)** â€“ fastest to start

  - Extract NP, VP, PP chunks
  - Record each chunkâ€™s `(start_token, end_token)` indices

#### 3ï¸âƒ£ Build alt-phrases inside each chunk

- For the token rows belonging to that chunk:

  1. Collect `top_logprobs` options
  2. Beam-recombine â‰¤ 2 token replacements
  3. **Score** every variant = sum of log-probs
  4. Keep top 3â€“5 ranked alternatives

_(This uses the data we already haveâ€”no extra API call.)_

#### 4ï¸âƒ£ Return JSON to UI

```json
{
  "sentence": "...",
  "chunks":[
    {"id":1,"label":"NP","text":"An 80-year-long study",
     "alts":["A study spanning 80 years","An eight-decade-long study"]},
    ...
  ]
}
```

#### 5ï¸âƒ£ Regenerate on user edit

- `/regenerate` endpoint

  - Freeze chosen chunk(s) in the prompt
  - Ask GPT to rewrite unlocked parts, again with `logprobs`
  - Go back to **2ï¸âƒ£** for the new draft

---

### ğŸ“‚ Suggested file layout

```
backend/
  â”œâ”€ openai_client.py          # wrapper: translate & regenerate
  â”œâ”€ chunker.py                # spaCy logic
  â”œâ”€ alt_builder.py            # log-prob beam recombination
  â”œâ”€ api.py                    # FastAPI routes
frontend/
  â””â”€ CAT_UI.jsx                # (later) Streamlit/React prototype
```

---

### ğŸ‘‰ Your call

- **Start with block 1** (OpenAI wrapper) and get a working JSON payload?
- **Jump to block 2** (spaCy chunker) to see chunk offsets in action?
- Or straight into **block 3** (alt-builder) to prove the log-prob trick?

Let me know which module you want first, and weâ€™ll code it line-by-line.
